Home_Sales
# ğŸ âœ¨ **Spark SQL Performance Analysis** âœ¨ğŸ 

## ğŸš© **Overview:**
This assignment demonstrates how to use **Apache Spark SQL** to efficiently handle big data, focusing on:
- Loading and querying data.
- Creating temporary views.
- Caching data for improved performance.
- Partitioning data and saving in Parquet format.
- Performance benchmarking.

---

## ğŸ”§ **Technologies Used:**

- âœ… **Apache Spark (v3.5.5)**
- âœ… **PySpark (Spark SQL)**
- âœ… **Google Colab**
- âœ… **Parquet Data Format**

---

## ğŸ“Œ **Dataset Used:**

- **home_sales_revised.csv**
- Contains home sales data with attributes such as date built, bedrooms, bathrooms, floors, square footage, view rating, and price.

---

## ğŸ“š **Steps Completed:**

### 1ï¸âƒ£ **Setup & Data Loading**
- Apache Spark session initialized.
- Loaded CSV data into PySpark DataFrame (`home_sales_df`).

### 2ï¸âƒ£ **Temporary View Creation**
- Created Spark temporary view (`home_sales`) for SQL queries.

### 3ï¸âƒ£ **Queries Performed:**
- **Average price per year** for 4-bedroom homes.
- **Average price per built-year** for homes with 3 bedrooms and 3 bathrooms.
- **Average price per built-year** for homes with 3 bedrooms, 3 bathrooms, 2 floors, and â‰¥2000 sqft.
- **Average price per view rating** (â‰¥ $350,000 average price).

### 4ï¸âƒ£ **Data Optimization Techniques:**
- **Caching** temporary views.
- **Partitioning** data by `date_built` and saving as Parquet.

---

## ğŸš€ **Performance Benchmark Results:**

| Method             | Runtime (seconds) â±ï¸ |
|--------------------|----------------------|
| ğŸ”´ **Uncached**    | `0.742`              |
| ğŸŸ¢ **Cached**      | `0.546` (**Fastest**) |
| ğŸ”µ **Partitioned** | `0.676`              |

### ğŸ“Š **Performance Insights:**
- **Caching** significantly reduces runtime due to memory optimization.
- **Partitioning** improves performance by efficiently accessing relevant data partitions, though slightly slower than fully cached data.
- **Uncached data** shows slowest performance due to repeated disk I/O operations.

---

## ğŸ¯ **Conclusions & Best Practices:**
- ğŸŸ¢ **Caching**: Best for repeated queries on the same dataset.
- ğŸ”µ **Partitioning**: Ideal for improving efficiency on larger datasets stored on disk.
- ğŸ› ï¸ Combining caching and partitioning can provide optimal query performance.

---

## âœ¨ **Final Thoughts:**
This exercise highlights how powerful Apache Spark SQL optimization techniques like caching and partitioning can drastically improve big-data processing performance. By strategically applying these methods, you can achieve efficient data analysis at scale.

ğŸŒŸ **Happy Sparking!** ğŸŒŸ

